{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "#!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118964,\n",
       " (\"Tom doesn't know if he will be able to visit us next Monday.\",\n",
       "  '[start]Tom no sabe si nos podrá visitar el próximo lunes.[end]'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = 'spa-eng/spa.txt'\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split('\\t')\n",
    "    spanish = '[start]' + spanish + '[end]'\n",
    "    text_pairs.append((english, spanish))\n",
    "\n",
    "import random\n",
    "len(text_pairs), random.choice(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83276, 17844, 17844)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "n_val_samples = int(0.15 * len(text_pairs))\n",
    "n_train_samples = len(text_pairs) - 2 * n_val_samples\n",
    "train_pairs = text_pairs[:n_train_samples]\n",
    "val_pairs = text_pairs[n_train_samples:(n_train_samples + n_val_samples)]\n",
    "test_pairs = text_pairs[-n_val_samples:]\n",
    "\n",
    "len(train_pairs), len(val_pairs), len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 12:27:10.250431: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 12:27:10.271873: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-26 12:27:10.272297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-26 12:27:10.790149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello i dont think'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_chars = string.punctuation + '¿'\n",
    "strip_chars = strip_chars.replace('[', '').replace(']', '')\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')\n",
    "\n",
    "custom_standardization(\"hello? I don't think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(21,), dtype=int64, numpy=array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>,\n",
       " <tf.Tensor: shape=(21,), dtype=int64, numpy=\n",
       " array([6649,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0])>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers, activations, optimizers, losses, metrics, callbacks\n",
    "\n",
    "vocab_size = 15000\n",
    "seq_len = 20\n",
    "\n",
    "eng_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode='int', output_sequence_length=seq_len)\n",
    "spa_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode='int', output_sequence_length=seq_len + 1, standardize=custom_standardization)\n",
    "\n",
    "train_eng_text = [pair[0] for pair in train_pairs]\n",
    "train_spa_text = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_text)\n",
    "spa_vectorization.adapt(train_spa_text)\n",
    "\n",
    "spa_vectorization('[start]'), spa_vectorization('[end]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'eng': <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "  array([[   3, 3093,    2, 1289,   11,  521,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0]])>,\n",
       "  'spa': <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "  array([[   1,    5, 1090,   40, 5951,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0]])>},\n",
       " <tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       " array([[   5, 1090,   40, 5951,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return ({\n",
    "        'eng': eng,\n",
    "        'spa': spa[:,:-1]\n",
    "    },\n",
    "    spa[:,1:])\n",
    "\n",
    "format_dataset([train_pairs[0][0]], [train_pairs[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 14:48:47.422625: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=16)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(x['eng'].shape)\n",
    "    print(x['spa'].shape)\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = layers.Input(shape=(None,), dtype=tf.int64, name='eng')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(layers.GRU(units=latent_dim), merge_mode='sum')(x)\n",
    "\n",
    "past_target = layers.Input(shape=(None,), dtype=tf.int64, name='spa')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(units=latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=activations.softmax)(x)\n",
    "seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 796s 607ms/step - loss: 5.4326 - sparse_categorical_accuracy: 0.2328 - val_loss: 4.5854 - val_sparse_categorical_accuracy: 0.3012\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 777s 597ms/step - loss: 4.4504 - sparse_categorical_accuracy: 0.3269 - val_loss: 3.9142 - val_sparse_categorical_accuracy: 0.3857\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 794s 610ms/step - loss: 3.8893 - sparse_categorical_accuracy: 0.3893 - val_loss: 3.4821 - val_sparse_categorical_accuracy: 0.4409\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 805s 618ms/step - loss: 3.4694 - sparse_categorical_accuracy: 0.4367 - val_loss: 3.1822 - val_sparse_categorical_accuracy: 0.4796\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 799s 613ms/step - loss: 3.1382 - sparse_categorical_accuracy: 0.4754 - val_loss: 2.9710 - val_sparse_categorical_accuracy: 0.5086\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 797s 612ms/step - loss: 2.8693 - sparse_categorical_accuracy: 0.5080 - val_loss: 2.8104 - val_sparse_categorical_accuracy: 0.5330\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 809s 621ms/step - loss: 2.6417 - sparse_categorical_accuracy: 0.5353 - val_loss: 2.6934 - val_sparse_categorical_accuracy: 0.5506\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 834s 641ms/step - loss: 2.4485 - sparse_categorical_accuracy: 0.5601 - val_loss: 2.6062 - val_sparse_categorical_accuracy: 0.5622\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 815s 626ms/step - loss: 2.2802 - sparse_categorical_accuracy: 0.5810 - val_loss: 2.5399 - val_sparse_categorical_accuracy: 0.5722\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 781s 600ms/step - loss: 2.1333 - sparse_categorical_accuracy: 0.6010 - val_loss: 2.4826 - val_sparse_categorical_accuracy: 0.5816\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 811s 623ms/step - loss: 2.0071 - sparse_categorical_accuracy: 0.6177 - val_loss: 2.4373 - val_sparse_categorical_accuracy: 0.5885\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 809s 621ms/step - loss: 1.8931 - sparse_categorical_accuracy: 0.6326 - val_loss: 2.4039 - val_sparse_categorical_accuracy: 0.5949\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 802s 616ms/step - loss: 1.7936 - sparse_categorical_accuracy: 0.6460 - val_loss: 2.3773 - val_sparse_categorical_accuracy: 0.5969\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 807s 620ms/step - loss: 1.7077 - sparse_categorical_accuracy: 0.6586 - val_loss: 2.3520 - val_sparse_categorical_accuracy: 0.6023\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 799s 613ms/step - loss: 1.6314 - sparse_categorical_accuracy: 0.6691 - val_loss: 2.3312 - val_sparse_categorical_accuracy: 0.6041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcb1426ded0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(optimizer=optimizers.RMSprop(), loss=losses.SparseCategoricalCrossentropy(), metrics=[metrics.SparseCategoricalAccuracy()])\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('seq2seq_rnn', save_best_only=True)\n",
    "]\n",
    "seq2seq_rnn.fit(train_ds, validation_data=val_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(21,), dtype=int64)\n",
      "-\n",
      "Were I in your position, I would oppose that plan.\n",
      "[start] en su lugar de que [UNK] su plan[end]            \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "print(spa_vectorization('[start]'))\n",
    "\n",
    "def decode_seq(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = '[start]'\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])\n",
    "        next_token_predict = seq2seq_rnn([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predict[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_token\n",
    "        if sampled_token == '[end]':\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(1):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-')\n",
    "    print(input_sentence)\n",
    "    print(decode_seq(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=activations.relu),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_casual_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
    "        i = tf.range(seq_len)[:, tf.newaxis]\n",
    "        j = tf.range(seq_len)\n",
    "        mask = tf.cast(i >= j, dtype=tf.int32)\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat([\n",
    "            tf.expand_dims(batch_size, -1),\n",
    "            tf.constant([1, 1], dtype=tf.int32)\n",
    "        ], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        casual_mask = self.get_casual_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            padding_mask = tf.minimum(padding_mask, casual_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=casual_mask\n",
    "        )\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,key=encoder_outputs,\n",
    "            attention_mask=padding_mask\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(None,), dtype=tf.int64, name='eng')\n",
    "x = PositionalEmbedding(seq_len, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=tf.int64, name='spa')\n",
    "x = PositionalEmbedding(seq_len, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=activations.softmax)(x)\n",
    "transformer = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - ETA: 0s - loss: 1.5387 - sparse_categorical_accuracy: 0.7839INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 384s 294ms/step - loss: 1.5387 - sparse_categorical_accuracy: 0.7839 - val_loss: 1.1970 - val_sparse_categorical_accuracy: 0.8118\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 1.1836 - sparse_categorical_accuracy: 0.8153INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 390s 300ms/step - loss: 1.1836 - sparse_categorical_accuracy: 0.8153 - val_loss: 1.0387 - val_sparse_categorical_accuracy: 0.8318\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 1.0606 - sparse_categorical_accuracy: 0.8303INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 389s 298ms/step - loss: 1.0606 - sparse_categorical_accuracy: 0.8303 - val_loss: 0.9620 - val_sparse_categorical_accuracy: 0.8429\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.9813 - sparse_categorical_accuracy: 0.8417INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 398s 306ms/step - loss: 0.9813 - sparse_categorical_accuracy: 0.8417 - val_loss: 0.9244 - val_sparse_categorical_accuracy: 0.8482\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.9203 - sparse_categorical_accuracy: 0.8510INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 404s 310ms/step - loss: 0.9203 - sparse_categorical_accuracy: 0.8510 - val_loss: 0.8883 - val_sparse_categorical_accuracy: 0.8543\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8743 - sparse_categorical_accuracy: 0.8581INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 405s 311ms/step - loss: 0.8743 - sparse_categorical_accuracy: 0.8581 - val_loss: 0.8702 - val_sparse_categorical_accuracy: 0.8590\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8386 - sparse_categorical_accuracy: 0.8642INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 404s 310ms/step - loss: 0.8386 - sparse_categorical_accuracy: 0.8642 - val_loss: 0.8600 - val_sparse_categorical_accuracy: 0.8603\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8057 - sparse_categorical_accuracy: 0.8699INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 405s 311ms/step - loss: 0.8057 - sparse_categorical_accuracy: 0.8699 - val_loss: 0.8387 - val_sparse_categorical_accuracy: 0.8657\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7689 - sparse_categorical_accuracy: 0.8763INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 406s 312ms/step - loss: 0.7689 - sparse_categorical_accuracy: 0.8763 - val_loss: 0.8216 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7391 - sparse_categorical_accuracy: 0.8813INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 404s 310ms/step - loss: 0.7391 - sparse_categorical_accuracy: 0.8813 - val_loss: 0.8025 - val_sparse_categorical_accuracy: 0.8729\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7130 - sparse_categorical_accuracy: 0.8858INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 402s 309ms/step - loss: 0.7130 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.7929 - val_sparse_categorical_accuracy: 0.8749\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.6920 - sparse_categorical_accuracy: 0.8894INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 404s 311ms/step - loss: 0.6920 - sparse_categorical_accuracy: 0.8894 - val_loss: 0.7858 - val_sparse_categorical_accuracy: 0.8767\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.6742 - sparse_categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 405s 311ms/step - loss: 0.6742 - sparse_categorical_accuracy: 0.8922 - val_loss: 0.7830 - val_sparse_categorical_accuracy: 0.8782\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.6592 - sparse_categorical_accuracy: 0.8949INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: full_transformer_decoder/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 406s 311ms/step - loss: 0.6592 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.7792 - val_sparse_categorical_accuracy: 0.8798\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 401s 308ms/step - loss: 0.6458 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.7815 - val_sparse_categorical_accuracy: 0.8805\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 402s 309ms/step - loss: 0.6344 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.7881 - val_sparse_categorical_accuracy: 0.8797\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 402s 309ms/step - loss: 0.6238 - sparse_categorical_accuracy: 0.9016 - val_loss: 0.7941 - val_sparse_categorical_accuracy: 0.8799\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 401s 308ms/step - loss: 0.6144 - sparse_categorical_accuracy: 0.9034 - val_loss: 0.7935 - val_sparse_categorical_accuracy: 0.8811\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 402s 309ms/step - loss: 0.6072 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.7992 - val_sparse_categorical_accuracy: 0.8817\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 402s 309ms/step - loss: 0.5992 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.8046 - val_sparse_categorical_accuracy: 0.8817\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 24621s 19s/step - loss: 0.5923 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.8055 - val_sparse_categorical_accuracy: 0.8824\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 391s 300ms/step - loss: 0.5856 - sparse_categorical_accuracy: 0.9092 - val_loss: 0.8132 - val_sparse_categorical_accuracy: 0.8822\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 398s 305ms/step - loss: 0.5802 - sparse_categorical_accuracy: 0.9102 - val_loss: 0.8143 - val_sparse_categorical_accuracy: 0.8830\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 406s 312ms/step - loss: 0.5753 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.8312 - val_sparse_categorical_accuracy: 0.8823\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 406s 312ms/step - loss: 0.5709 - sparse_categorical_accuracy: 0.9120 - val_loss: 0.8262 - val_sparse_categorical_accuracy: 0.8827\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 395s 304ms/step - loss: 0.5663 - sparse_categorical_accuracy: 0.9131 - val_loss: 0.8340 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 410s 315ms/step - loss: 0.5619 - sparse_categorical_accuracy: 0.9140 - val_loss: 0.8443 - val_sparse_categorical_accuracy: 0.8826\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 411s 316ms/step - loss: 0.5576 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.8482 - val_sparse_categorical_accuracy: 0.8834\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 409s 314ms/step - loss: 0.5532 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.8532 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 410s 315ms/step - loss: 0.5502 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.8581 - val_sparse_categorical_accuracy: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fca8f147610>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(optimizer=optimizers.RMSprop(), loss=losses.SparseCategoricalCrossentropy(), metrics=[metrics.SparseCategoricalAccuracy()])\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('full_transformer_decoder', save_best_only=True)\n",
    "]\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 21), dtype=int64)\n",
      "-\n",
      "There's no water coming out of the shower.\n",
      "[start] no hay agua de la ducha[end]              \n",
      "[start] no hay agua de la ducha[end]              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_vocab = spa_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "print(spa_vectorization(['[start]']))\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = '[start]'\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = spa_vectorization([decoded_sentence])\n",
    "        tokenized_target_sentence = tokenized_target_sentence[:, :-1] # length of sequence must be 20 (with padding) to match input\n",
    "        predictions = transformer(inputs=[tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_token\n",
    "        if sampled_token == '[end]':\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(1):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-')\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n",
    "    print(decode_seq(input_sentence))\n",
    "\n",
    "spa_vocab.index('[UNK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
