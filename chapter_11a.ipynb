{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "#!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xf aclImdb_v1.tar.gz\n",
    "#!rm -r aclImdb/train/unsup\n",
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "if False:\n",
    "    base_dir = pathlib.Path('aclImdb')\n",
    "    val_dir = base_dir / 'val'\n",
    "    train_dir = base_dir / 'train'\n",
    "    for category in ('neg', 'pos'):\n",
    "        os.makedirs(val_dir / category)\n",
    "        files = os.listdir(train_dir / category)\n",
    "        random.Random(1337).shuffle(files)\n",
    "        n_val_samples = int(0.2 * len(files))\n",
    "        val_files = files[-n_val_samples:]\n",
    "        for filename in val_files:\n",
    "            shutil.move(train_dir / category / filename, val_dir / category / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 17:46:45.557629: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-20 17:46:45.681652: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-20 17:46:45.682495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 17:46:46.293512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 17:46:47.374022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-20 17:46:47.374326: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,) <dtype: 'string'> tf.Tensor(b\"Movies like these are to the originals what Album Oriented Rock stations are to what music used to be like - repetitive, boring, and drained of all the original energy by a committee of corporate drones. I AM glad that Aragorn wasn't typecast as an expectant psycho by this P.O.S. Go back and watch the 1971 version, count the things that would NEVER be included in a modern version, and thank whatever deity you worship that someone somewhere in the distant past had the balls to write and shoot an original concept movie that wasn't based on someone else's ideas, and wasn't passed through a corporate board before it saw the light of day.\", shape=(), dtype=string)\n",
      "(32,) <dtype: 'int32'> tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(inputs.shape, inputs.dtype, inputs[0])\n",
    "    print(targets.shape, targets.dtype, targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "text_vectorization = layers.TextVectorization(max_tokens=20000, output_mode='multi_hot')\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20000) <dtype: 'float32'> tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "(32,) <dtype: 'int32'> tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(inputs.shape, inputs.dtype, inputs[0])\n",
    "    print(targets.shape, targets.dtype, targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, activations, optimizers, losses, metrics, callbacks\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = layers.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=activations.relu)(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=activations.sigmoid)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizers.RMSprop(), loss=losses.BinaryCrossentropy(), metrics=[metrics.BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "604/625 [===========================>..] - ETA: 0s - loss: 0.4028 - binary_accuracy: 0.8285INFO:tensorflow:Assets written to: binary_1gram/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: binary_1gram/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4001 - binary_accuracy: 0.8302 - val_loss: 0.2825 - val_binary_accuracy: 0.8898\n",
      "Epoch 2/10\n",
      "594/625 [===========================>..] - ETA: 0s - loss: 0.2708 - binary_accuracy: 0.9003INFO:tensorflow:Assets written to: binary_1gram/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: binary_1gram/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2716 - binary_accuracy: 0.9000 - val_loss: 0.2777 - val_binary_accuracy: 0.8952\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2403 - binary_accuracy: 0.9128 - val_loss: 0.2928 - val_binary_accuracy: 0.8914\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2233 - binary_accuracy: 0.9247 - val_loss: 0.3053 - val_binary_accuracy: 0.8878\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2185 - binary_accuracy: 0.9294 - val_loss: 0.3260 - val_binary_accuracy: 0.8854\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2107 - binary_accuracy: 0.9312 - val_loss: 0.3333 - val_binary_accuracy: 0.8862\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2031 - binary_accuracy: 0.9322 - val_loss: 0.3437 - val_binary_accuracy: 0.8822\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2056 - binary_accuracy: 0.9362 - val_loss: 0.3542 - val_binary_accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2013 - binary_accuracy: 0.9385 - val_loss: 0.3618 - val_binary_accuracy: 0.8868\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2074 - binary_accuracy: 0.9378 - val_loss: 0.3734 - val_binary_accuracy: 0.8822\n",
      "782/782 [==============================] - 1s 663us/step - loss: 0.2896 - binary_accuracy: 0.8886\n",
      "[0.28956401348114014, 0.8885999917984009]\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('binary_1gram', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_1gram_train_ds, epochs=10, validation_data=binary_1gram_val_ds, callbacks=callback_list)\n",
    "\n",
    "model = keras.models.load_model('binary_1gram')\n",
    "print(model.evaluate(binary_1gram_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(ngrams=2, max_tokens=20000, output_mode='multi_hot')\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "589/625 [===========================>..] - ETA: 0s - loss: 0.3843 - binary_accuracy: 0.8400INFO:tensorflow:Assets written to: binary_2gram/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: binary_2gram/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3790 - binary_accuracy: 0.8430 - val_loss: 0.2621 - val_binary_accuracy: 0.9002\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2370 - binary_accuracy: 0.9137 - val_loss: 0.2653 - val_binary_accuracy: 0.9028\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1951 - binary_accuracy: 0.9334 - val_loss: 0.2740 - val_binary_accuracy: 0.9084\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1808 - binary_accuracy: 0.9416 - val_loss: 0.2884 - val_binary_accuracy: 0.9050\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1681 - binary_accuracy: 0.9497 - val_loss: 0.3068 - val_binary_accuracy: 0.9026\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1600 - binary_accuracy: 0.9516 - val_loss: 0.3227 - val_binary_accuracy: 0.9044\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1488 - binary_accuracy: 0.9555 - val_loss: 0.3337 - val_binary_accuracy: 0.9042\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1513 - binary_accuracy: 0.9562 - val_loss: 0.3524 - val_binary_accuracy: 0.9040\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1517 - binary_accuracy: 0.9584 - val_loss: 0.3557 - val_binary_accuracy: 0.8984\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.1466 - binary_accuracy: 0.9584 - val_loss: 0.3704 - val_binary_accuracy: 0.8994\n",
      "782/782 [==============================] - 1s 594us/step - loss: 0.2687 - binary_accuracy: 0.8969\n",
      "[0.2687144875526428, 0.8969200253486633]\n"
     ]
    }
   ],
   "source": [
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "\n",
    "model = get_model()\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('binary_2gram', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds, validation_data=binary_2gram_val_ds, epochs=10, callbacks=callback_list)\n",
    "\n",
    "model = keras.models.load_model('binary_2gram')\n",
    "print(model.evaluate(binary_2gram_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = layers.TextVectorization(ngrams=2, max_tokens=20000, output_mode='tf_idf')\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "616/625 [============================>.] - ETA: 0s - loss: 0.4953 - binary_accuracy: 0.7837INFO:tensorflow:Assets written to: tfidf_2gram/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfidf_2gram/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4939 - binary_accuracy: 0.7846 - val_loss: 0.3002 - val_binary_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "612/625 [============================>.] - ETA: 0s - loss: 0.3247 - binary_accuracy: 0.8656INFO:tensorflow:Assets written to: tfidf_2gram/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfidf_2gram/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3249 - binary_accuracy: 0.8660 - val_loss: 0.2950 - val_binary_accuracy: 0.8984\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2810 - binary_accuracy: 0.8880 - val_loss: 0.3147 - val_binary_accuracy: 0.8978\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2534 - binary_accuracy: 0.8967 - val_loss: 0.3345 - val_binary_accuracy: 0.8986\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2442 - binary_accuracy: 0.9011 - val_loss: 0.3695 - val_binary_accuracy: 0.8890\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2297 - binary_accuracy: 0.9069 - val_loss: 0.3443 - val_binary_accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2165 - binary_accuracy: 0.9107 - val_loss: 0.3641 - val_binary_accuracy: 0.8958\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2110 - binary_accuracy: 0.9151 - val_loss: 0.3662 - val_binary_accuracy: 0.8928\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2048 - binary_accuracy: 0.9173 - val_loss: 0.3837 - val_binary_accuracy: 0.8840\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2053 - binary_accuracy: 0.9194 - val_loss: 0.3826 - val_binary_accuracy: 0.8810\n",
      "782/782 [==============================] - 1s 598us/step - loss: 0.2959 - binary_accuracy: 0.8910\n",
      "[0.29592445492744446, 0.891040027141571]\n"
     ]
    }
   ],
   "source": [
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('tfidf_2gram', save_best_only=True)\n",
    "]\n",
    "\n",
    "model = get_model()\n",
    "model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds, epochs=10, callbacks=callback_list)\n",
    "\n",
    "model = keras.models.load_model('tfidf_2gram')\n",
    "print(model.evaluate(tfidf_2gram_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = layers.TextVectorization(max_tokens=max_tokens, output_mode='int', output_sequence_length=max_length)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot_3 (TFOpLambda)   (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                5128448   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5128513 (19.56 MB)\n",
      "Trainable params: 5128513 (19.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.5624 - binary_accuracy: 0.7060INFO:tensorflow:Assets written to: one_hot_bidir_lstm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_hot_bidir_lstm/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 2141s 3s/step - loss: 0.5624 - binary_accuracy: 0.7060 - val_loss: 0.3725 - val_binary_accuracy: 0.8498\n",
      "782/782 [==============================] - 889s 1s/step - loss: 0.3902 - binary_accuracy: 0.8344\n",
      "[0.39024680852890015, 0.8343600034713745]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=tf.int64)\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=activations.sigmoid)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=optimizers.RMSprop(), loss=losses.BinaryCrossentropy(), metrics=[metrics.BinaryAccuracy()])\n",
    "model.summary()\n",
    "\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('one_hot_bidir_lstm', save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callback_list)\n",
    "\n",
    "model = keras.models.load_model('one_hot_bidir_lstm')\n",
    "print(model.evaluate(int_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot_7 (TFOpLambda)   (None, None, 20000)       0         \n",
      "                                                                 \n",
      " separable_conv1d_9 (Separa  (None, None, 64)          1340064   \n",
      " bleConv1D)                                                      \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " separable_conv1d_10 (Separ  (None, None, 64)          4352      \n",
      " ableConv1D)                                                     \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, None, 64)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1344481 (5.13 MB)\n",
      "Trainable params: 1344481 (5.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.6932 - binary_accuracy: 0.4979INFO:tensorflow:Assets written to: one_hot_conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_hot_conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 719s 1s/step - loss: 0.6932 - binary_accuracy: 0.4979 - val_loss: 0.6932 - val_binary_accuracy: 0.5000\n",
      "782/782 [==============================] - 362s 463ms/step - loss: 0.6932 - binary_accuracy: 0.5000\n",
      "[0.6931540369987488, 0.5]\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=tf.int64)\n",
    "x = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.SeparableConv1D(filters=64, kernel_size=3, activation=activations.relu)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.SeparableConv1D(filters=64, kernel_size=3, activation=activations.relu)(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=activations.sigmoid)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=optimizers.RMSprop(), loss=losses.BinaryCrossentropy(), metrics=[metrics.BinaryAccuracy()])\n",
    "model.summary()\n",
    "\n",
    "callback_list = [\n",
    "    callbacks.ModelCheckpoint('one_hot_conv', save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callback_list)\n",
    "\n",
    "model = keras.models.load_model('one_hot_conv')\n",
    "print(model.evaluate(int_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
